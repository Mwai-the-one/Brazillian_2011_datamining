# ğŸ›ï¸ DSA2040A_DataMining_EcomAnalytics  
### *Uncovering Hidden Patterns in Brazilâ€™s E-Commerce World* ğŸ‡§ğŸ‡·  

---

## ğŸŒŸ Project Overview

Welcome to our **end-to-end data mining adventure!**  
In this project, we dive deep into the **Brazilian E-Commerce Public Dataset by Olist**, exploring how real customers shop, how sellers perform, and what drives customer satisfaction in Brazilâ€™s growing online marketplace.  

Our mission is to transform messy, real-world data into **actionable insights** â€” moving through every stage of the pipeline:  
**ETL â†’ EDA â†’ Data Mining â†’ Insights & Storytelling.**

We aim to answer *real business questions*, build machine learning models, and visualize insights that help e-commerce platforms like Olist **make smarter, data-driven decisions.** ğŸ’¡

---

## ğŸ¯ Objectives & Research Questions

Our analysis focuses on solving key business problems through data-driven insights:

### ğŸ‘©â€ğŸ’» Customer Insights
- Who are Olistâ€™s most valuable customers?  
- Which **regions** in Brazil drive the most sales and revenue?  
- Can we segment customers based on purchase frequency or spending?

### ğŸ“¦ Product & Sales Analysis
- Which **product categories** are top performers?  
- How does **seasonality** affect order volume?  
- What are the most preferred **payment methods** and how do they affect purchase value?

### â­ Customer Experience
- What factors influence **review scores**?  
- Do **late deliveries** lower satisfaction levels?  
- Are there **correlations** between order value, payment type, and ratings?

### ğŸ§  Predictive & Pattern Discovery
- Can we **predict** customer satisfaction levels?  
- Which products are **frequently bought together** (Market Basket Analysis)?  

---


### Members contribution
## Week 2 member 2 Gachunga Gift
Detailed Week 2 Contribution Summary

During Week 2, significant progress was made in building a full data preparation pipeline, starting from raw Excel data to clean, structured, encoded datasets ready for mining tasks. The work involved ETL development, data cleaning, missing value handling, outlier detection, feature engineering, scaling, and monthly one-hot encoding.

The process began with the creation of an ETL pipeline that loaded the raw Excel dataset, standardized column names, and converted key fields such as InvoiceDate into the proper datetime format. Invalid rows containing zero or negative Quantity or UnitPrice were removed, eliminating 11,699 faulty transactions. The pipeline provided clear validation feedback by printing the datasetâ€™s initial and final shapes (from 547,328 rows down to 514,256) and returned a cleaned DataFrame that served as the foundation for the rest of the processing.

Further cleaning included removing 8,065 duplicate records to ensure data consistency. Missing values were carefully handled: missing country names were replaced with "Unknown", rows missing critical identifiers such as InvoiceDate, InvoiceNo, or StockCode were removed entirely, and missing CustomerID fields were filled with -1 so those transactions could still be used analytically. Negative quantities were preserved but flagged using a new IsReturn column to distinguish returned items. Country naming inconsistencies (like â€œRSAâ€, â€œEIREâ€, â€œEuropean Communityâ€) were standardized to more accurate and uniform names. Additional analytical features were engineered, including TotalPrice and date-based fields such as Year, Month, and Day derived from InvoiceDate. The cleaned dataset was then exported to cleaned_data.csv.

Outlier detection was performed using the Interquartile Range (IQR) method. Outliers in Quantity and UnitPrice were identified, with clear upper and lower bounds calculated for each numeric variable. Sample outlier records were displayed to provide context on the data points that fell outside acceptable thresholds. Multiple strategies for handling these outliers were implemented: removing extreme values entirely, applying Winsorization to cap values within IQR limits, and eliminating records with zero or negative prices. These different approaches resulted in several alternative cleaned datasets (df_clean, df_capped, df_no_zero_price), offering flexibility for subsequent analysis and model selection.

To prepare the data for market basket mining, scaling was applied to Quantity and UnitPrice using MinMaxScaler to ensure consistent ranges for algorithms sensitive to value magnitude. The dataset was then segmented by month to allow month-specific transaction pattern analysis. For each month, transactions were reconstructed by grouping products under each invoice, creating realistic item lists per transaction. The TransactionEncoder was used to convert these item lists into one-hot encoded matrices, with sparse matrix handling implemented to preserve memory efficiency. The encoded datasets for each month were saved separately, such as encoded_products_Month_11.csv for November and encoded_products_Month_12.csv for December, providing ready-to-use inputs for Apriori and association rule mining in later stages.

Overall, the Week 2 work produced a comprehensive, well-cleaned, validated, and fully encoded dataset pipeline that transforms raw retail data into structured, analysis-ready inputs suitable for advanced data mining processes.

## ğŸ§¾ Dataset Description

**ğŸ“š Source:** [Brazilian E-Commerce Public Dataset by Olist (Kaggle)](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)  
**ğŸ“Š Data Period:** 2016â€“2018  

**ğŸ“¦ Files Included:**

| File | Description |
|------|--------------|
| `olist_orders_dataset.csv` | Order details and delivery statuses |
| `olist_order_items_dataset.csv` | Products sold per order |
| `olist_products_dataset.csv` | Product category metadata |
| `olist_cu_
```
DSA2040A_DataMining_EcomAnalytics/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original Olist CSV files
â”‚ â”œâ”€â”€ transformed/ # Cleaned and merged datasets
â”‚ â””â”€â”€ final/ # Ready-to-analyze datasets
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 1_extract_transform.ipynb
â”‚ â”œâ”€â”€ 2_exploratory_analysis.ipynb
â”‚ â”œâ”€â”€ 3_data_mining.ipynb
â”‚ â””â”€â”€ 4_insights_dashboard.ipynb
â”œâ”€â”€ report/
â”‚ â”œâ”€â”€ executive_summary.pdf
â”‚ â””â”€â”€ presentation.pptx
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

```
---

## ğŸ‘¥ Team Members & Roles

| ğŸ§‘â€ğŸ’» Name | ğŸ“ Role | ğŸ” Responsibilities |
|------------|----------|---------------------|
| *[Member 1]* | **ETL Lead** | Data extraction, cleaning, and transformation |
| *[Member 2]* | **Data Analyst** | EDA, visualization, and summary statistics |
| *[Member 3]* | **Data Miner / Modeler** | Clustering, classification, and association rules |
| *[Member 4]* | **Reporter / Presenter** | Dashboard creation, executive summary, presentation |
| *All Members* | **Collaborators** | Each makes â‰¥3 commits and documents their notebook sections |

---

## ğŸ§° Tools & Technologies

ğŸ–¥ï¸ **Programming:** Python (Pandas, NumPy, scikit-learn)  
ğŸ“Š **Visualization:** Seaborn, Matplotlib, Plotly, Power BI  
ğŸ¤– **Machine Learning:** K-Means, Logistic Regression, Decision Tree, Apriori  
ğŸ“ **Version Control:** Git & GitHub (team collaboration)  
ğŸ§¾ **Reporting:** Jupyter Notebooks, PDF Summary, PowerPoint Slides  

---

## ğŸš€ Expected Outcomes

By the end of this project, we will have:  
âœ… A **clean and enriched dataset** ready for analysis  
ğŸ“ˆ A **detailed exploratory analysis** with visual insights  
ğŸ¤– **Machine learning models** for prediction and clustering  
ğŸ›ï¸ **Actionable business insights** for e-commerce strategy  
ğŸ¨ A **mini dashboard** for storytelling and presentation  

This project showcases how **data science turns raw transactions into strategic intelligence** â€” helping businesses grow smarter, faster, and stronger! ğŸ’ªğŸ“Š  

---

Ultimately, this project demonstrates how data science transforms raw transactions into strategic intelligence â€” one dataset at a time. ğŸ”¥
